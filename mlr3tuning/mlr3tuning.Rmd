---
title: "mlr3tuning"
author: "mlr3-org"
date: "`r format(Sys.time(), '%d %B, %Y')`"
short_description: "Hyperparameter Tuning with mlr3tuning"
logo: "logo.png"
column_breaks: [3, 5, 7]
output: 
  cheatdown::cheatdown_html
---

```{r, include=FALSE}
library(mlr3)
library(mlr3tuning)
library(mlr3learners)
library(data.table)
library(paradox)
```

## Class Overview

The package provides a set of R6 classes which allow to (a) define general hyperparameter (HP) tuning instances and (b) run algorithms which optimize on these. 
(a) is called a `TuningInstanceSingleCrit` or `TuningInstaneMultiCrit`, which define a blackbox optimization function that maps HP candidate configurations to resampled performance values for arbitrary performance measures.

![](class_diagram.png)

## ParamSet - Parameters and Ranges

Scalar doubles, integers, factors or logicals are combined to define a multivariate tuning space.

```{r, eval=FALSE}
tune_ps = ps(
  <id> = p_int(lower, upper),
  <id> = p_dbl(lower, upper),
  <id> = p_dct(levels),
  <id> = p_lgl())
```

`id` is the Param identifier. `lower`/`upper` define numerical ranges, `levels` is for categories.

```{r, results='hide'}
lrn("classif.rpart", 
  cp = to_tune(0.001, 0.1, logscale = TRUE))
```

Alternatively, use `to_tune()` function to set tuning space in parameter set of the `Learner`. 
Set `logscale = TRUE` to tune on logarithmic scale.

## Terminators - When to stop

Construction: `trm(.key, ...)`

* `evals` (`n_evals`)<br> After a given amount of iterations.
* `run_time` (`secs `)<br> After a given training time.
* `perf_reached` (`level`)<br> After a specific performance was reached.
* `stagnation` (`iters`, `threshold`)<br> After the performance stagnated for given iterations.

```{r, results='hide'}
as.data.table(mlr_terminators)
```

Lists all available terminators.

## TuningInstance* - Search Scenario

Evaluator and container for resampled performances of HP configurations during tuning.
The main (internal) function `eval_batch(xdt)` calls `benchmark()` to evaluate a table of HP configurations.
Also stores archive of all evaluated experiments and the final result.

```{r, include=FALSE}
task = tsk("iris")
learner = lrn("classif.rpart")
resampling = rsmp("holdout")
measure = msr("classif.ce")
terminator = trm("evals", n_evals = 20)

tune_ps = ps(cp = p_dbl(0.001, 0.1, logscale = TRUE))
```

```{r, results='hide'}
instance = TuningInstanceSingleCrit$new(task, 
  learner, resampling, measure,terminator, tune_ps)
```

Set `store_benchmark_result = TRUE` to store resamplings of evaluations and `store_models = TRUE` to store associated models.

```{r, results='hide', class.source='example'}
# optimize hyperparameter of RBF SVM on logscale
learner = lrn("classif.svm", kernel = "radial", type = "C-classification")
    
tune_ps = ps(
  cost = p_dbl(1e-4, 1e4, logscale = TRUE),
  gamma = p_dbl(1e-4, 1e4, logscale = TRUE))

evals20 = trm("evals", n_evals = 20)
    
instance = TuningInstanceSingleCrit$new(task, learner, resampling, measure, evals20,
  tune_ps)
tuner = tnr("random_search")
tuner$optimize(instance)
instance$result
```

Use `TuningInstanceMultiCrit` for multi-criteria tuning.

## Tuner - Search Strategy

Tuning strategy. 
Generates candidate configurations and passes these to `TuningInstance` for evaluation until termination.
Creation: `tnr(.key, ...)`

* `grid_search` (`resolution`, `batch_size`) <br> Grid search.
* `random_search` (`batch_size`) <br> Random search.
* `gensa` (`smooth`, `temperature`) <br> Generalized Simulated Annealing.
* `nloptr` (`algorithm`) <br> Non-linear optimization.
* `irace` <br> Iterated racing.
* `design_points` (`batch_size `, `design`) <br> User supplied settings.

```{r, results='hide'}
as.data.table(mlr_tuners)
```
Lists all available tuners.

## Executing the Tuning

```{r, results='hide'}
tuner$optimize(instance)
```

Starts the tuning. `Tuner` generates candidate configurations and passes these to the `$eval_batch()` method of the `TuningInstance*` until the budget of the `Terminator` is exhausted.

```{r, results='hide'}
as.data.table(instance$archive)
```

Returns all evaluated configurations and their resampling results.
The `x_domain_*` columns contain HP values after the transformation.

```{r, results='hide', class.source='example'}
as.data.table(instance$archive)
## >     cost gamma classif.ce     uhash x_domain_cost x_domain_gamma
## > 1:  3.13  5.55       0.56  b8744...          3.13           5.55
## > 2: -1.94  1.32       0.10  f5623...         -1.94           1.32
```

`uhash` refers to `instance$archive$benchmark_result`.

```{r, results='hide'}
instance$result
```

Returns list with optimal configurations and estimated performance.
							
```{r, results='hide'}
learner$param_set$values = 
  instance$result_learner_param_vals
```

Set optimized HP in `Learner`.

```{r, results='hide', class.source='example'}
learner = lrn("classif.svm", type = "C-classification", kernel = "radial",
  cost = to_tune(1e-4, 1e4, logscale = TRUE), 
  gamma = to_tune(1e-4, 1e4, logscale = TRUE))

instance = tune(method = "grid_search", task = tsk("iris"), learner = learner, 
  resampling = rsmp ("holdout"), measure = msr("classif.ce"), resolution = 5)
```

Use `tune()`-shortcut.

## Logging and Parallelization

```{r, eval=FALSE}
lgr::get_logger("bbotk")$set_threshold("<level>")
```

Change log-level only for mlr3tuning.

```{r, eval=FALSE}
future::plan(strategy)
```

Sets the parallelization backend.
Speeds up tuning by running iterations in parallel.

## AutoTuner - Tune before Train

Wraps learner and performs integrated tuning.
							
```{r, results='hide'}
at = AutoTuner$new(learner, resampling, measure,
  terminator, tuner)
```

Inherits from class `Learner`.
Training starts tuning on the training set.
After completion the learner is trained with the "optimal" configuration on the given task.

```{r, include=FALSE}
row_ids = 1:50
```

```{r, results='hide'}
at$train(task)
at$predict(task, row_ids)
```

```{r, results='hide'}
at = auto_tuner(method = "grid_search", learner,
  resampling, measure, term_evals = 20)
```

Use shortcut to create `AutoTuner`.

## Nested Resampling

Resampling the `AutoTuner` results in nested resampling with an inner and outer loop.
							
```{r, results='hide', class.source='example'}
inner_resampling = rsmp("holdout")

at = auto_tuner(method = "random_search", learner, inner_resampling, 
  measure, term_evals = 20) 
									
outer_resampling = rsmp("cv", folds = 2)
rr = resample(task, at, outer_resampling, store_models = TRUE)
									
as.data.table(rr)
## >             learner         resampling iteration  
## > 1:  <AutoTuner[37]> <ResamplingCV[19]>         1
## > 2:  <AutoTuner[37]> <ResamplingCV[19]>         2
```

```{r, results='hide'}
extract_inner_tuning_results(rr)
```

Check inner tuning results for stable HPs.

```{r, results='hide'}
rr$score()
```

Predictive performances estimated on the outer resampling.

```{r, results='hide'}
extract_inner_tuning_archives(rr)
```

All evaluated HP configurations.

```{r, results='hide'}
rr$aggregate()
```

Aggregates performances of outer resampling iterations. 
						
```{r, results='hide'}
rr = tune_nested(method = "grid_search", task, 
  learner = learner, inner_resampling, outer_resampling, measure,
  term_evals = 20, )
```

Use shortcut to execute nested resampling.
