\documentclass{beamer}

\usepackage[orientation=landscape,size=a0,scale=1.4,debug]{beamerposter}
\mode<presentation>{\usetheme{mlr}}

\usepackage[sfdefault]{roboto}
\usepackage{roboto-mono}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc} % UTF-8
\usepackage[english]{babel} % Language
\usepackage{hyperref} % Hyperlinks
\usepackage{ragged2e} % Text position
\usepackage[export]{adjustbox} % Image position
\usepackage[most]{tcolorbox} % Code boxes
\usepackage{multido}

\hypersetup{
	hyperfootnotes=false,
	colorlinks=true,
	linktocpage=true,
	pdfauthor={mlr-org team},
	%linkcolor=[RGB]{3,99,142}, % mlr blue
	urlcolor=[RGB]{231,138,69}
}

\title{Hyperparameter Tuning with mlr3tuning :\,: CHEAT SHEET} % Package title in header, \, adds thin space between ::

\newlength{\columnheight} % Adjust depending on header height
\setlength{\columnheight}{84cm} 

\newtcolorbox{codebox}{%
	sharp corners,
	leftrule=0pt,
	rightrule=0pt,
	toprule=0pt,
	bottomrule=0pt,
	fontupper=\robotomono\small,
	hbox}

\newtcolorbox{codeboxmultiline}[1][]{%
	sharp corners,
	leftrule=0pt,
	rightrule=0pt,
	toprule=0pt,
	bottomrule=0pt,
	fontupper=\robotomono\small,
	#1}

\newtcolorbox{codeboxexample}{%
	sharp corners,
	leftrule=0pt,
	rightrule=0pt,
	toprule=0pt,
	bottomrule=0pt,
	fontupper=\robotomono\small,
	width=27cm,
	adjusted title=Example,
	fonttitle = \bfseries\Large,
	top = 0.5em}

\newtcolorbox{codeboxinline}{%
	sharp corners,
	leftrule=0pt,
	rightrule=0pt,
	toprule=0pt,
	bottomrule=0pt,
	hbox,
	nobeforeafter,
	fontupper=\robotomono\small,
	tcbox raise base}

\newcommand{\codeinline}[1]{\begin{codeboxinline}#1\end{codeboxinline}}
\newcommand{\sectionheading}[1]{{\color{mlrblue}\large\raggedright\textbf{#1}}\vspace{1em}}
\newcommand{\monospace}[1]{\multido{}{#1}{\space}}

\begin{document}
\begin{frame}[fragile]{}
	\begin{columns}
		\begin{column}{.245\textwidth}
			\begin{beamercolorbox}[center]{postercolumn}
				\begin{minipage}{.98\textwidth}
					\parbox[t][\columnheight]{\textwidth}{
						\begin{myblock}{Class Overview}
							The package provides a set of R6 classes which allow to (a) define general
							hyperparameter (HP) tuning instances and (b) run algorithms which optimize on these.
							(a) is called a TuningInstanceSingleCrit or TuningInstaneMultiCrit, which define a blackbox optimization function that maps HP candidate configurations to resampled performance values for arbitrary performance measures.\\
							\\
							\includegraphics[width=0.95\textwidth]{img/class_diagram.pdf}
						\end{myblock}
						\begin{myblock}{ParamSet - Parameters and Ranges}
							% Hyperparameters and ranges for tuning (1). 
							Scalar doubles, integers, factors or logicals are combined to define a multivariate tuning space.
							\\
							\begin{codeboxmultiline}[width=20.75cm]
								tune\_ps = \textbf{ParamSet}\$new(list(\\
								\hspace*{1ex}\textbf{ParamInt}\$new(id, lower, upper),\\
								\hspace*{1ex}\textbf{ParamDbl}\$new(id, lower, upper),\\
								\hspace*{1ex}\textbf{ParamFct}\$new(id, levels),\\
								\hspace*{1ex}\textbf{ParamLgl}\$new(id)))
							\end{codeboxmultiline}
							\codeinline{id} is the Param identifier.
							\codeinline{lower}/\codeinline{upper} define numerical ranges, \codeinline{levels} is for categories.
							\vspace{1em}
							\\
							\sectionheading{Transformations for Rescaling}
							\begin{codeboxmultiline}[width=25.5cm]
								tune\_ps\$\textbf{trafo} = function(x, param\_set) \{\\
								\hspace*{1ex}x\$id =  2\^{}x\$id; return(x)\}
							\end{codeboxmultiline}
							Apply a custom transformation before passing the param to the \codeinline{Learner}.
							\vspace{1em}
							\\
							\sectionheading{Parameter Dependencies}
							\\
							Dependencies prevent invalid learner configurations.
							\\
							\begin{codebox}
								tune\_ps\$\textbf{add\_dep}(id, on, cond)
							\end{codebox}
							Adds a dependency for param \codeinline{id} so that param \codeinline{id} depends on param \codeinline{on}, optional to condition \codeinline{cond}.
							%\codeinline{cond} is \codeinline{CondEqual\$new(rhs)} or \codeinline{CondAnyOf\$new(rhs)}.
						\end{myblock}
						\vfill}
				\end{minipage}
			\end{beamercolorbox}
		\end{column}
		\begin{column}{.245\textwidth}
			\begin{beamercolorbox}[center]{postercolumn}
				\begin{minipage}{.98\textwidth}
					\parbox[t][\columnheight]{\textwidth}{
						\begin{myblock}{Terminators - When to stop}
							Construction: \codeinline{\textbf{trm}(.key, ...)}
							\\
							\begin{itemize}
								\item \codeinline{evals}
								      (\codeinline{n\_evals})\\
								      After a given amount of iterations.
								\item \codeinline{clock\_time}
								      (\codeinline{secs}, \codeinline{stop\_time})\\
								      After a given absolute time.
								\item \codeinline{model\_time}
								      (\codeinline{secs })\\
								      After a given training time.
								\item \codeinline{perf\_reached}
								      (\codeinline{level})\\
								      After a specific performance was reached.
								\item \codeinline{stagnation}
								      (\codeinline{iters}, \codeinline{threshold})\\
								      After the performance stagnated for given iterations.
							\end{itemize}
							\vspace{1em}
							\begin{codebox}
								as.data.table(\textbf{mlr\_terminators})
							\end{codebox}
							Lists all available terminators.
							% \begin{codebox}
							% 	terminator = term("\textbf{combo}", terminators, any)
							% \end{codebox}
							% List of \codeinline{terminators} that terminate
							% if any (\codeinline{any = TRUE})
							% or all (\codeinline{any = FALSE}) terminators are positive.
						\end{myblock}
						\begin{myblock}{TuningInstance* - Search Scenario}
							Evaluator and container for resampled performances of HP configurations during tuning.
							The main (internal) function \codeinline{eval\_batch(xdt)} calls \codeinline{benchmark()} to evaluate a table of HP configurations.
							% If the available budget is exhausted, an exception is raised, and no further evaluations can be performed from this point on. 
							Also stores archive of all evaluated experiments and the final result.
							\\
							\begin{codeboxmultiline}[width=25cm]
								instance = \textbf{TuningInstanceSingleCrit}\$new(\\
								\hspace*{1ex}task, learner, resampling, measure,\\
								\hspace*{1ex}tune\_ps, terminator)
							\end{codeboxmultiline}
							\vspace{1em}
							\begin{codeboxexample}
								{\scriptsize
									\# optimize hyperpar of RBF SVM on logscale\\
									learner = lrn("classif.svm", kernel = "radial",\\
									\hspace*{1ex} type = "C-classification")\\
									\\
									tune\_ps = ParamSet\$new(list(\\
									\hspace*{1ex} ParamDbl\$new("cost", lower = -8, upper = 8),\\
									\hspace*{1ex} ParamDbl\$new("gamma", lower = -8, upper = 8)))\\
									tune\_ps\$trafo = function(x, param\_set) \{\\
									\hspace*{1ex} x\$cost = 2\textasciicircum x\$cost; x\$gamma = 2\textasciicircum x\$gamma; x\}\\
									evals20 = trm("evals", n\_evals = 20)\\
									\\
									instance = TuningInstanceSingleCrit\$new(\\
									\hspace*{1ex} task, learner, resampling, measure, tune\_ps, evals20)\\
									tuner = tnr("random\_search")\\
									tuner\$optimize(instance)\\
									instance\$result}
							\end{codeboxexample}
							Use \codeinline{TuningInstanceMultiCrit} for multi-criteria tuning.
						\end{myblock}
						\vfill}
				\end{minipage}
			\end{beamercolorbox}
		\end{column}
		\begin{column}{.245\textwidth}
			\begin{beamercolorbox}[center]{postercolumn}
				\begin{minipage}{.98\textwidth}
					\parbox[t][\columnheight]{\textwidth}{
						\begin{myblock}{Tuner - Search Strategy}
							Tuning strategy. 
							Generates candidate configurations and passes these to \codeinline{TuningInstance} for evaluation until termination.
							Creation: \codeinline{\textbf{tnr}(.key, ...)}
							\\
							\begin{itemize}
								\item \codeinline{grid\_search}
								      (\codeinline{resolution}, \codeinline{batch\_size})\\
								      Grid search.
								\item \codeinline{random\_search}
								      (\codeinline{batch\_size})\\
								      Random search.
								\item \codeinline{gensa}
								      (\codeinline{smooth}, \codeinline{temperature})\\
								      Generalized Simulated Annealing.
								\item \codeinline{nloptr}
									  (\codeinline{algorithm})\\
									  Non-linear optimization.
								\item \codeinline{design\_points}
								      (\codeinline{batch\_size }, \codeinline{design})\\
								      User supplied settings.
							\end{itemize}
							\vspace{1em}
							\begin{codebox}
								as.data.table(\textbf{mlr\_tuners})
							\end{codebox}
							Lists all available tuners.
						\end{myblock}
						\begin{myblock}{Executing the Tuning}
							\begin{codebox}
								tuner\$\textbf{optimize}(instance)
							\end{codebox}
							Starts the tuning. \codeinline{Tuner} generates candidate configurations and passes these to the \codeinline{\$eval\_batch()} method of the \codeinline{TuningInstance*} until the budget of the \codeinline{Terminator} is exhausted.
							\\
							\begin{codebox}
								instance\$archive\$\textbf{data()}
							\end{codebox}
							Returns all evaluated configurations and their resampling results.
							Use \codeinline{unnest} to display HP with (\codeinline{x\_domain}) trafo applied.
							\\
							\begin{codeboxmultiline}[width=23.3cm]
								{\tiny
									instance\$archive\$data()\\
									\#\#\monospace{4}cost\monospace{2}gamma classif.ce  resample\_result \monospace{1}x\_domain batch\_nr\\
									\#\# 1: -1.01 -3.10 0.10\monospace{7}<ResampleResult> <list>\monospace{3}1\\
									\#\# 2:  7.53\monospace{2}3.14\monospace{2}0.08\monospace{7}<ResampleResult> <list>\monospace{3}2\\
									\#\# 3:  2.11\monospace{2}6.50\monospace{2}0.68\monospace{7}<ResampleResult> <list>\monospace{3}3\\
									\#\# 4:  4.25\monospace{2}5.54\monospace{2}0.52\monospace{7}<ResampleResult> <list>\monospace{3}4\\
								}
							\end{codeboxmultiline}
							\vspace{1em}
							\begin{codebox}
								instance\$\textbf{result}
							\end{codebox}
							Returns list with optimal configurations and estimated performance.
							\\
							\begin{codeboxmultiline}[width=18.2cm]
								{\footnotesize learner\$param\_set\$\textbf{values} = \\ instance\$\textbf{result\_learner\_param\_vals}}
							\end{codeboxmultiline}
							Set optimized HP in \codeinline{Learner}.
						\end{myblock}
						\vfill}
				\end{minipage}
			\end{beamercolorbox}
		\end{column}
		\begin{column}{.245\textwidth}
			\begin{beamercolorbox}[center]{postercolumn}
				\begin{minipage}{.98\textwidth}
					\parbox[t][\columnheight]{\textwidth}{
						\begin{myblock}{AutoTuner - Tune before Train}
							Wraps learner and performs integrated tuning.
							\\
							\begin{codeboxmultiline}[width=24.5cm]
								at = \textbf{AutoTuner}\$new(\\
								\hspace*{1ex}learner, resampling, measure, tune\_ps, \\
								\hspace*{1ex}terminator, tuner)
							\end{codeboxmultiline}
							\vspace{0.5em}
							Inherits from class \codeinline{Learner}.
							Training starts tuning on the training set.
							After completion the learner is trained with the "optimal" configuration on the given task.
							\\
							\begin{codeboxmultiline}[width=16.5cm]
								at\$\textbf{train}(task)\\
								at\$\textbf{predict}(task, row\_ids)
							\end{codeboxmultiline}
						\end{myblock}
						\begin{myblock}{Nested Resampling}
							Resampling the \codeinline{AutoTuner} results in nested resampling with an inner and outer loop.
							\\
							\begin{codeboxexample}
								{\scriptsize
									resampling\_inner = rsmp("holdout")
									\vspace{1em}
									\\
									at = AutoTuner\$new(learner, resampling\_inner, \\
									\hspace*{1ex}measure, tune\_ps, evals20, tuner) \\
									at\$store\_tuning\_instance = TRUE
									\vspace{1em}
									\\
									resampling\_outer = rsmp("cv", folds = 2)\\
									rr = resample(task, at, resampling\_outer, \\
									\hspace*{1ex}store\_models = TRUE)
									\vspace{1em}
									\\
									rr\$data\\
									\#\# ...\monospace{3}learner\monospace{5}resampling iteration
									prediction\\
									\#\# ... <AutoTuner> <ResamplingCV>\monospace{9}1\monospace{5}<list>\\
									\#\# ... <AutoTuner> <ResamplingCV>\monospace{9}2\monospace{5}<list>}
							\end{codeboxexample}
							\vspace{1em}
							\begin{codebox}
								rr\$\textbf{aggregate()}
							\end{codebox}
							Aggregates performances of outer folds.
							\\
							\begin{codebox}
								rr\$data\$learner[[1]]\$\textbf{tuning\_result}
							\end{codebox}
							Retrieves inner tuning results.
							\vspace{-0.5em}
						\end{myblock}
						\begin{myblock}{Logging and Parallelization}
							\begin{codebox}
								{\scriptsize
									lgr::get\_logger("\textbf{bbotk}")\$set\_threshold("<level>")}
							\end{codebox}
							Change log-level only for mlr3tuning.\\
							\begin{codebox}
								future::\textbf{plan}(strategy)
							\end{codebox}
							Sets the parallelization backend.
							Speeds up tuning by running iterations in parallel.
						\end{myblock}
						\vfill}
				\end{minipage}
			\end{beamercolorbox}
		\end{column}
	\end{columns}
\end{frame}
\end{document}
